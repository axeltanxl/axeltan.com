---
layout: post
title: "Apple's Efforts To Tackle Child Sexual Abuse"
---

Apple recently announced plans to [introduce more child protection features](https://www.apple.com/child-safety/) in the upcoming version of its operating systems (iOS, macOS, iPadOS & watchOS). The features include:

1. Notifying parents whenever their child views or sends sexually explicit material

2. Providing guidance and advice whenever a CSAM-related (Child Sexual Abuse Material) query is made in Siri or Search.

3. Scanning of users' iCloud photos to detect CSAM material

As you can probably guess, the most controversial feature is the scanning of user's photos. According to Apple, the novel technology will scan images to be uploaded to iCloud. It will scan images on the user's device and compare it to known CSAM images provided by the National Center for Missing and Exploited Children (NCMEC) in the US. However, it will not scan the images in pictorial form. Instead, it will compare the hashes of CSAM images with hashes of the image to find a match. Image hashes are a unique combination of numbers and letters associated with each image. More importantly, its employees are not allowed to view the photos directly unless the technology repeatedly detects CSAM material in a user's account. Once that happens, Apple employees will manually review the images and file a report to NCMEC if they deem necessary.

This well-intentioned feature has sparked huge debate and resistance, as many users see this as a step backwards for privacy. Following this, Apple have decided to push back the release of these features, in order to gather more feedback to make the necessary improvements.

These days, any privacy-related announcement is bound to come under huge public scrutiny. Remember when WhatsApp updated their privacy policy earlier this year to inform users about how they plan to share information with its parent company? That also didn't turn out well. Yes, child sexual abuse is a serious issue to needs to be addressed. And I get that Apple wants to trailblaze and do their part to counter child predators. However, they should consider the interests of the larger majority of its consumers who aren't child predators. They should have at least sought public opinion before deciding on implementing this feature. Nobody wants their phones to be constantly scanned. Until they can come up with less privacy-intrusive solutions to deal with child sexual abuse, I believe that detection of these crimes should be left to law enforcement officers. With the release of the new iPhones and operating systems now imminent, this controversy could potentially hurt their sales.